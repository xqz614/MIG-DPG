#!/usr/bin/env python3
"""
MIG-DPG å°è§„æ¨¡è®­ç»ƒå®éªŒ
éªŒè¯å®Œæ•´çš„è®­ç»ƒæµç¨‹å’Œæ¨¡å‹æ€§èƒ½
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import sys
import os
import time
import warnings
from torch.utils.data import DataLoader
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

def create_simple_model():
    """åˆ›å»ºç®€åŒ–çš„MIG-DPGæ¨¡å‹"""
    
    class SimpleMIG_DPG(nn.Module):
        def __init__(self, num_users, num_items, embedding_dim=64, hidden_dim=128):
            super().__init__()
            
            # åµŒå…¥å±‚
            self.user_embedding = nn.Embedding(num_users, embedding_dim)
            self.item_embedding = nn.Embedding(num_items, embedding_dim)
            
            # æ¨èé¢„æµ‹å™¨
            self.recommendation_predictor = nn.Sequential(
                nn.Linear(embedding_dim * 2, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, 1),
                nn.Sigmoid()
            )
            
            # DPOå±‚ï¼ˆç®€åŒ–ç‰ˆï¼‰
            self.dpo_predictor = nn.Sequential(
                nn.Linear(embedding_dim * 2, hidden_dim),
                nn.ReLU(), 
                nn.Linear(hidden_dim, 1)
            )
            
            # ç”Ÿæˆå±‚ï¼ˆç®€åŒ–ç‰ˆï¼‰
            self.generation_predictor = nn.Sequential(
                nn.Linear(embedding_dim * 2, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, 100),  # ç®€åŒ–è¯æ±‡è¡¨
                nn.Softmax(dim=-1)
            )
            
        def forward(self, users, items, mode='recommendation'):
            user_emb = self.user_embedding(users)
            item_emb = self.item_embedding(items)
            user_item = torch.cat([user_emb, item_emb], dim=-1)
            
            if mode == 'recommendation':
                return self.recommendation_predictor(user_item)
            elif mode == 'dpo':
                return self.dpo_predictor(user_item)
            elif mode == 'generation':
                return self.generation_predictor(user_item)
            else:
                rec_score = self.recommendation_predictor(user_item)
                dpo_score = self.dpo_predictor(user_item)
                gen_probs = self.generation_predictor(user_item)
                return rec_score, dpo_score, gen_probs
        
        def get_all_scores(self, users):
            """è·å–ç”¨æˆ·å¯¹æ‰€æœ‰ç‰©å“çš„è¯„åˆ†"""
            user_emb = self.user_embedding(users)  # [batch_size, embedding_dim]
            item_emb = self.item_embedding.weight  # [num_items, embedding_dim]
            
            # è®¡ç®—æ‰€æœ‰ç”¨æˆ·-ç‰©å“å¯¹çš„åˆ†æ•°
            scores = torch.mm(user_emb, item_emb.t())  # [batch_size, num_items]
            return torch.sigmoid(scores)
    
    return SimpleMIG_DPG

def create_synthetic_dataset():
    """åˆ›å»ºåˆæˆæ•°æ®é›†"""
    
    class SyntheticDataset:
        def __init__(self, num_users=200, num_items=100, num_interactions=2000):
            self.num_users = num_users
            self.num_items = num_items
            
            # ç”Ÿæˆç”¨æˆ·-ç‰©å“äº¤äº’
            self.interactions = []
            
            # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆåå¥½æ¨¡å¼
            user_preferences = {}
            for user in range(num_users):
                # æ¯ä¸ªç”¨æˆ·åå¥½5-15ä¸ªç‰©å“ç±»åˆ«
                preferred_items = np.random.choice(num_items, 
                                                 np.random.randint(5, 16), 
                                                 replace=False)
                user_preferences[user] = set(preferred_items)
            
            # ç”Ÿæˆäº¤äº’ï¼ˆ80%æ­£å‘ï¼Œ20%è´Ÿå‘ï¼‰
            for _ in range(num_interactions):
                user = np.random.randint(0, num_users)
                
                if np.random.random() < 0.8:  # æ­£å‘äº¤äº’
                    if user in user_preferences:
                        item = np.random.choice(list(user_preferences[user]))
                        rating = np.random.choice([4, 5])  # é«˜è¯„åˆ†
                    else:
                        item = np.random.randint(0, num_items)
                        rating = np.random.choice([3, 4])
                else:  # è´Ÿå‘äº¤äº’
                    item = np.random.randint(0, num_items)
                    rating = np.random.choice([1, 2])  # ä½è¯„åˆ†
                
                self.interactions.append([user, item, rating])
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            self.interactions = np.array(self.interactions)
            
            # åˆ†å‰²æ•°æ®é›†
            np.random.shuffle(self.interactions)
            train_size = int(0.8 * len(self.interactions))
            val_size = int(0.1 * len(self.interactions))
            
            self.train_data = self.interactions[:train_size]
            self.val_data = self.interactions[train_size:train_size+val_size]
            self.test_data = self.interactions[train_size+val_size:]
            
            print(f"æ•°æ®é›†ç»Ÿè®¡:")
            print(f"  ç”¨æˆ·æ•°: {num_users}, ç‰©å“æ•°: {num_items}")
            print(f"  æ€»äº¤äº’æ•°: {len(self.interactions)}")
            print(f"  è®­ç»ƒé›†: {len(self.train_data)}")
            print(f"  éªŒè¯é›†: {len(self.val_data)}")
            print(f"  æµ‹è¯•é›†: {len(self.test_data)}")
        
        def get_dataloader(self, split='train', batch_size=64, shuffle=True):
            if split == 'train':
                data = self.train_data
            elif split == 'val':
                data = self.val_data
            else:
                data = self.test_data
            
            class SimpleDataset:
                def __init__(self, interactions, num_items):
                    self.interactions = interactions
                    self.num_items = num_items
                
                def __len__(self):
                    return len(self.interactions)
                
                def __getitem__(self, idx):
                    user, item, rating = self.interactions[idx]
                    
                    # è´Ÿé‡‡æ ·
                    neg_item = np.random.randint(0, self.num_items)
                    while neg_item == item:
                        neg_item = np.random.randint(0, self.num_items)
                    
                    return {
                        'user': user,
                        'pos_item': item,
                        'neg_item': neg_item,
                        'rating': rating
                    }
            
            dataset = SimpleDataset(data, self.num_items)
            return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
    
    return SyntheticDataset()

def compute_metrics(model, dataloader, device, topk=10):
    """è®¡ç®—æ¨èæŒ‡æ ‡"""
    model.eval()
    
    all_users = []
    all_pos_items = []
    all_scores = []
    
    with torch.no_grad():
        for batch in dataloader:
            users = batch['user'].to(device)
            pos_items = batch['pos_item'].to(device)
            
            # è·å–ç”¨æˆ·å¯¹æ‰€æœ‰ç‰©å“çš„è¯„åˆ†
            scores = model.get_all_scores(users)
            
            all_users.extend(users.cpu().numpy())
            all_pos_items.extend(pos_items.cpu().numpy())
            all_scores.append(scores.cpu())
    
    all_scores = torch.cat(all_scores, dim=0)
    
    # è®¡ç®—Recall@Kå’ŒNDCG@K
    recall_sum = 0
    ndcg_sum = 0
    num_users = len(all_users)
    
    for i in range(num_users):
        user_scores = all_scores[i]
        pos_item = all_pos_items[i]
        
        # è·å–Top-Kæ¨è
        _, topk_items = torch.topk(user_scores, topk)
        topk_items = topk_items.numpy()
        
        # Recall@K
        if pos_item in topk_items:
            recall_sum += 1
            
            # NDCG@K
            pos_rank = np.where(topk_items == pos_item)[0][0] + 1
            ndcg_sum += 1.0 / np.log2(pos_rank + 1)
    
    recall = recall_sum / num_users
    ndcg = ndcg_sum / num_users
    
    return recall, ndcg

def train_experiment():
    """è¿è¡Œå®Œæ•´è®­ç»ƒå®éªŒ"""
    print("ğŸš€ å¼€å§‹MIG-DPGå°è§„æ¨¡è®­ç»ƒå®éªŒ")
    print("=" * 60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ•°æ®é›†
    print("\nğŸ“Š åˆ›å»ºåˆæˆæ•°æ®é›†...")
    dataset = create_synthetic_dataset()
    
    train_loader = dataset.get_dataloader('train', batch_size=32, shuffle=True)
    val_loader = dataset.get_dataloader('val', batch_size=64, shuffle=False)
    test_loader = dataset.get_dataloader('test', batch_size=64, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ§  åˆå§‹åŒ–æ¨¡å‹...")
    ModelClass = create_simple_model()
    model = ModelClass(dataset.num_users, dataset.num_items).to(device)
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    rec_criterion = nn.BCELoss()
    dpo_criterion = nn.MSELoss()
    gen_criterion = nn.CrossEntropyLoss()
    
    # è®­ç»ƒé…ç½®
    num_epochs = 20
    log_interval = 50
    best_val_recall = 0
    
    print(f"\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒ (å…±{num_epochs}ä¸ªepochs)...")
    
    train_losses = []
    val_recalls = []
    val_ndcgs = []
    
    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0
        batch_count = 0
        
        start_time = time.time()
        
        for batch_idx, batch in enumerate(train_loader):
            users = batch['user'].to(device)
            pos_items = batch['pos_item'].to(device)
            neg_items = batch['neg_item'].to(device)
            ratings = batch['rating'].float().to(device)
            
            optimizer.zero_grad()
            
            # æ¨èæŸå¤± (BPR-like)
            pos_scores = model(users, pos_items, mode='recommendation').squeeze()
            neg_scores = model(users, neg_items, mode='recommendation').squeeze()
            rec_loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()
            
            # DPOæŸå¤± (ç®€åŒ–ç‰ˆ)
            pos_dpo = model(users, pos_items, mode='dpo').squeeze()
            neg_dpo = model(users, neg_items, mode='dpo').squeeze()
            dpo_loss = dpo_criterion(pos_dpo, ratings/5.0) + dpo_criterion(neg_dpo, torch.zeros_like(neg_dpo))
            
            # ç”ŸæˆæŸå¤± (ç®€åŒ–ç‰ˆ)
            gen_targets = torch.randint(0, 100, (len(users),)).to(device)
            gen_probs = model(users, pos_items, mode='generation')
            gen_loss = gen_criterion(gen_probs, gen_targets)
            
            # æ€»æŸå¤±
            total_loss = rec_loss + 0.3 * dpo_loss + 0.2 * gen_loss
            
            total_loss.backward()
            optimizer.step()
            
            epoch_loss += total_loss.item()
            batch_count += 1
            
            if batch_idx % log_interval == 0:
                print(f"Epoch {epoch+1:2d} [{batch_idx:3d}/{len(train_loader):3d}] "
                      f"Loss: {total_loss.item():.4f} "
                      f"(Rec: {rec_loss.item():.4f}, DPO: {dpo_loss.item():.4f}, Gen: {gen_loss.item():.4f})")
        
        avg_train_loss = epoch_loss / batch_count
        train_losses.append(avg_train_loss)
        
        # éªŒè¯
        if (epoch + 1) % 5 == 0:
            print(f"\nğŸ” éªŒè¯ Epoch {epoch+1}...")
            val_recall, val_ndcg = compute_metrics(model, val_loader, device, topk=10)
            val_recalls.append(val_recall)
            val_ndcgs.append(val_ndcg)
            
            print(f"éªŒè¯ç»“æœ: Recall@10={val_recall:.4f}, NDCG@10={val_ndcg:.4f}")
            
            if val_recall > best_val_recall:
                best_val_recall = val_recall
                print(f"âœ… æ–°çš„æœ€ä½³éªŒè¯Recall: {best_val_recall:.4f}")
        
        epoch_time = time.time() - start_time
        print(f"Epoch {epoch+1:2d} å®Œæˆï¼Œç”¨æ—¶: {epoch_time:.2f}sï¼Œå¹³å‡æŸå¤±: {avg_train_loss:.4f}\n")
    
    # æœ€ç»ˆæµ‹è¯•
    print("ğŸ¯ æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
    test_recall, test_ndcg = compute_metrics(model, test_loader, device, topk=10)
    
    print(f"\nğŸ“ˆ æœ€ç»ˆç»“æœ:")
    print(f"  æµ‹è¯• Recall@10: {test_recall:.4f}")
    print(f"  æµ‹è¯• NDCG@10: {test_ndcg:.4f}")
    print(f"  æœ€ä½³éªŒè¯ Recall@10: {best_val_recall:.4f}")
    
    # åˆ†æè®­ç»ƒæ›²çº¿
    print(f"\nğŸ“Š è®­ç»ƒåˆ†æ:")
    print(f"  åˆå§‹è®­ç»ƒæŸå¤±: {train_losses[0]:.4f}")
    print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.4f}")
    print(f"  æŸå¤±ä¸‹é™: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%")
    
    if val_recalls:
        print(f"  éªŒè¯Recallæå‡: {((val_recalls[-1] - val_recalls[0]) / val_recalls[0] * 100 if val_recalls[0] > 0 else 0):.1f}%")
    
    return {
        'train_losses': train_losses,
        'val_recalls': val_recalls,
        'val_ndcgs': val_ndcgs,
        'test_recall': test_recall,
        'test_ndcg': test_ndcg,
        'best_val_recall': best_val_recall
    }

def main():
    """ä¸»å‡½æ•°"""
    torch.manual_seed(42)
    np.random.seed(42)
    
    try:
        results = train_experiment()
        
        print("\nğŸ‰ å®éªŒå®Œæˆï¼")
        print("=" * 60)
        print("å®éªŒç»“æœå·²ä¿å­˜ï¼Œå¯ä»¥è¿›è¡Œè¿›ä¸€æ­¥åˆ†æã€‚")
        
        return results
        
    except Exception as e:
        print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    results = main() 